{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3aa09286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch, requests, json, random, re, config\n",
    "\n",
    "# # Use model directly without downloading\n",
    "# g1_model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\") \n",
    "# g2_model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\n",
    "# g1_tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "# g2_tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\n",
    "\n",
    "# # save models\n",
    "# g1_model.save_pretrained(\"./models/g1_model\")\n",
    "# g2_model.save_pretrained(\"./models/g2_model\")\n",
    "\n",
    "# # save tokenizer\n",
    "# g1_tokenizer.save_pretrained(\"./models/g1_tokenizer\")\n",
    "# g2_tokenizer.save_pretrained(\"./models/g2_tokenizer\")\n",
    "\n",
    "# Now you can load the models and tokenizer from the saved files\n",
    "g1_model = AutoModelForSeq2SeqLM.from_pretrained(\"./models/g1_model/\")\n",
    "g2_model = AutoModelForSeq2SeqLM.from_pretrained(\"./models/g2_model/\")\n",
    "g1_tokenizer = AutoTokenizer.from_pretrained(\"./models/g1_tokenizer/\")\n",
    "g2_tokenizer = AutoTokenizer.from_pretrained(\"./models/g2_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e8247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g1_model.eval()\n",
    "g2_model.eval()\n",
    "g1_model.to(device)\n",
    "g2_model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def question_generation_sampling(\n",
    "    g1_model,\n",
    "    g1_tokenizer,\n",
    "    g2_model,\n",
    "    g2_tokenizer,\n",
    "    context,\n",
    "    num_questions,\n",
    "    device,\n",
    "):\n",
    "    qa_input_ids = prepare_qa_input(\n",
    "            g1_tokenizer,\n",
    "            context=context,\n",
    "            device=device,\n",
    "    )\n",
    "    max_repeated_sampling = int(num_questions * 3) # sometimes generated question+answer is invalid\n",
    "    num_valid_questions = 0\n",
    "    questions = []\n",
    "    for q_ in range(max_repeated_sampling):\n",
    "        # Stage G.1: question+answer generation\n",
    "        outputs = g1_model.generate(\n",
    "            qa_input_ids,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        question_answer = g1_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        question_answer = question_answer.replace(g1_tokenizer.pad_token, \"\").replace(g1_tokenizer.eos_token, \"\")\n",
    "        question_answer_split = question_answer.split(g1_tokenizer.sep_token)\n",
    "        if len(question_answer_split) == 2:\n",
    "            # valid Question + Annswer output\n",
    "            num_valid_questions += 1\n",
    "        else:\n",
    "            continue\n",
    "        question = question_answer_split[0].strip()\n",
    "        answer = question_answer_split[1].strip()\n",
    "\n",
    "        # Stage G.2: Distractor Generation\n",
    "        distractor_input_ids = prepare_distractor_input(\n",
    "            g2_tokenizer,\n",
    "            context = context,\n",
    "            question = question,\n",
    "            answer = answer,\n",
    "            device = device,\n",
    "            separator = g2_tokenizer.sep_token,\n",
    "        )\n",
    "        outputs = g2_model.generate(\n",
    "            distractor_input_ids,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        distractors = g2_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        distractors = distractors.replace(g2_tokenizer.pad_token, \"\").replace(g2_tokenizer.eos_token, \"\")\n",
    "        distractors = re.sub(\"<extra\\S+>\", g2_tokenizer.sep_token, distractors)\n",
    "        distractors = [y.strip() for y in distractors.split(g2_tokenizer.sep_token)]\n",
    "        options = [answer] + distractors\n",
    "\n",
    "        while len(options) < 4:\n",
    "            options.append(options[-1])\n",
    "\n",
    "        question_item = {\n",
    "            'question': question,\n",
    "            'options': options,\n",
    "        }\n",
    "        questions.append(question_item)\n",
    "        if num_valid_questions == num_questions:\n",
    "            break\n",
    "    return questions\n",
    "\n",
    "\n",
    "def prepare_qa_input(t5_tokenizer, context, device):\n",
    "    \"\"\"\n",
    "    input: context\n",
    "    output: question <sep> answer\n",
    "    \"\"\"\n",
    "    encoding = t5_tokenizer(\n",
    "        [context],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def prepare_distractor_input(t5_tokenizer, context, question, answer, device, separator='<sep>'):\n",
    "    \"\"\"\n",
    "    input: question <sep> answer <sep> article\n",
    "    output: distractor1 <sep> distractor2 <sep> distractor3\n",
    "    \"\"\"\n",
    "    input_text = question + ' ' + separator + ' ' + answer + ' ' + separator + ' ' + context\n",
    "    encoding = t5_tokenizer(\n",
    "        [input_text],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    return input_ids\n",
    "\n",
    "def generate_multiple_choice_question(\n",
    "    context\n",
    "):\n",
    "    num_questions = 1\n",
    "    question_item = question_generation_sampling(\n",
    "        g1_model, g1_tokenizer,\n",
    "        g2_model, g2_tokenizer,\n",
    "        context, num_questions, device\n",
    "    )[0]\n",
    "    question = question_item['question']\n",
    "    options = question_item['options']\n",
    "    answer = options[0]\n",
    "    random.shuffle(options) # shuffle options\n",
    "    question_json = {\n",
    "        'question': question,\n",
    "        'options': options,\n",
    "        'answers': options[0]\n",
    "    }\n",
    "    return json.dumps(question_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324a74e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"question\": \"Which mountain ranges are considered to form Asia\\'s western border?\", \"options\": [\"Ural Mountains, the Caucasus Mountains, and the Caspian and Black seas\", \"Ural Mountains and the Caucasus Mountains\", \"Ural Mountains, the Caucasus Mountains, and the Caspian and Black Seas\", \"Ural Mountains, the Caspian and Black Seas\"], \"answers\": \"Ural Mountains, the Caucasus Mountains, and the Caspian and Black seas\"}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_multiple_choice_question(\n",
    "    context = \" However, most geographers define Asia’s western border as an indirect line that follows the Ural Mountains, the Caucasus Mountains, and the Caspian and Black Seas.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dc21cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the text into sentences.\n",
    "\n",
    "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead \n",
    "    to incorrect splitting because they are used as markers for splitting.\n",
    "\n",
    "    :param text: text to be split into sentences\n",
    "    :type text: str\n",
    "\n",
    "    :return: list of sentences\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    alphabets = \"([A-Za-z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "    digits = \"([0-9])\"\n",
    "    multiple_dots = r'\\.{2,}'\n",
    "\n",
    "    # Preprocess text\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    text = re.sub(digits + \"[.]\" + digits, \"\\\\1<prd>\\\\2\", text)\n",
    "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    if sentences and not sentences[-1]:\n",
    "        sentences = sentences[:-1]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def combine_sentences(text, word_limit=33):\n",
    "    \"\"\"\n",
    "    Combine sentences into chunks based on a word limit.\n",
    "\n",
    "    :param text: text containing sentences\n",
    "    :type text: str\n",
    "    :param word_limit: maximum number of words in each chunk, defaults to 33\n",
    "    :type word_limit: int, optional\n",
    "    :return: list of chunks\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    # Combine sentences into chunks with a word limit\n",
    "    while sentences:\n",
    "        chunk = sentences.pop(0)\n",
    "        while len(chunk.split()) < word_limit and sentences:\n",
    "            chunk += ' ' + sentences.pop(0)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_context_and_question(text, num_q=10):\n",
    "    \"\"\"\n",
    "    Generate multiple-choice questions based on text chunks.\n",
    "\n",
    "    :param text: input text\n",
    "    :type text: str\n",
    "    :param num_q: number of questions to generate, defaults to 10\n",
    "    :type num_q: int, optional\n",
    "    :return: list of generated questions\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    quiz_content = []\n",
    "    chunks = combine_sentences(text)\n",
    "    random.shuffle(chunks) #Randomly select content\n",
    "    # Generate multiple-choice questions from text chunks\n",
    "    for i in range(num_q):\n",
    "        chunk = chunks[i % len(chunks)]\n",
    "        tmp = generate_multiple_choice_question(chunk)  # Function to generate question\n",
    "        print(tmp)\n",
    "        quiz_content.append(tmp)\n",
    "    \n",
    "    return quiz_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d2d59e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"How much of the global population lives in Asia?\", \"options\": [\"30 percent\", \"20 percent\", \"60 percent\", \"30 percent\"], \"answers\": \"30 percent\"}\n",
      "{\"question\": \"What region is to the south of Asia?\", \"options\": [\"Caucasus Mountains\", \"Ural Mountains\", \"Pacific\", \"Black Sea\"], \"answers\": \"Caucasus Mountains\"}\n",
      "{\"question\": \"How many countries are there in the Asia region?\", \"options\": [\"four major geographical regions\", \"three major physical regions\", \"five major geographic regions\", \"five major physical regions\"], \"answers\": \"four major geographical regions\"}\n",
      "{\"question\": \"What is the population of Asia?\", \"options\": [\"60 percent\", \"30 percent\", \"20 percent\", \"50 percent\"], \"answers\": \"60 percent\"}\n",
      "{\"question\": \"What continent occupies the western portion of the Eurasian supercontinent?\", \"options\": [\"Asia\", \"Africa\", \"Europe\", \"South America\"], \"answers\": \"Asia\"}\n"
     ]
    }
   ],
   "source": [
    "quiz_content = get_context_and_question(config.text_1, num_q=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
